{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 5, Lab 3: Between Subjects Experiments\n",
    "=============================================\n",
    "\n",
    "In this lab, we will explore the between-subjects experiment (sometimes\n",
    "referred to as an \"A/B\" test). I apply concepts from across the entire\n",
    "course, including review of statistical modeling, power analysis, effect\n",
    "size, measurement, and so on ... so that you can see the entire process\n",
    "at once.\n",
    "\n",
    "In this example, a design firm has produced several different logos for\n",
    "a company as part of a rebranding process, and the company executives\n",
    "have selected three that they like. These were then tested on a sample\n",
    "of 100 customers. Each participant completed a survey about a randomly\n",
    "selected logo. You are the data analyst and were called upon to make\n",
    "sense of the data.\n",
    "\n",
    "As part of the survey, participants rated their impression of the logo\n",
    "on several dimensions: \"friendly\", \"inviting\", \"interesting\",\n",
    "\"positive\", \"pleasant\" on a 1-10 scale (1 = *does not describe logo*; 10\n",
    "= *describes logo perfectly*). As these are all positive adjectives and\n",
    "you suspect people will not be very discerning in their answers, you\n",
    "wish to combine them into a single \"positive sentiment\" scale. You then\n",
    "wish to compare the logos on this sentiment scale.\n",
    "\n",
    "We will use the `ggplot2` package for data visualization, the `effsize`\n",
    "package for effect sizes,the `pwr` package for power analysis, and the\n",
    "`psych` package for reliability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD PACKAGES ####\n",
    "install.packages('pwr')\n",
    "install.packages('psych')\n",
    "install.packages('effsize')\n",
    "library(ggplot2)\n",
    "library(pwr)\n",
    "library(psych)\n",
    "library(effsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data\n",
    "=============\n",
    "\n",
    "We first load the data and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "dat <- read.csv(\"datasets/logos.csv\", header=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly glance at the names of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  We have an ID variable representing the individual participants\n",
    "2.  We have variables representing the different adjectives on which\n",
    "    people rated the logo\n",
    "3.  We have participant sex\n",
    "4.  We have the logo that was rated\n",
    "\n",
    "It is clear this is not the whole dataset, as there would likely have\n",
    "been more variables.\n",
    "\n",
    "Let's briefly check the structure of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily for us, we have the adjectives all scored as numeric variables\n",
    "and the `logo` variable scored as a factor. We will need to do little\n",
    "(or nothing) to wrangle the data.\n",
    "\n",
    "We do a final quick inspection on the variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the ranges on the adjective variables are all in range\n",
    "(1-10), which we should expect (but should still check). The `logo` was\n",
    "randomly assigned to participants, so this is approximately evenly\n",
    "distributed. Annoyingly, we have a missing value on the `logo` variable.\n",
    "Of note,`sex` weights toward females, which we assume is representative\n",
    "of the data source. Hopefully this is representative of likely customers\n",
    "(if not, this would be a key limitation of this study).\n",
    "\n",
    "Score the Scale\n",
    "===============\n",
    "\n",
    "We had the goal of creating a \"positive sentiment\" variable for these\n",
    "data. We can quickly eyeball a correlation matrix to ensure these\n",
    "measures correlate highly with each other. The adjectives represent\n",
    "columns 2-6 of `dat`, so the correlations among the adjectives can be\n",
    "shown with `cor(dat[,2:6])`. Remember that the brackets after `dat`\n",
    "contain a comma: the blank *before* the comma means \"all rows,\" the\n",
    "\"2:6\" after the comma means \"columns 2-6\". Note that I wrap this within\n",
    "`round()` to enhance readability, rounding to two decimals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(\n",
    "  cor(dat[,2:6])\n",
    ",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see these ratings are all moderately-strongly correlated. Later in\n",
    "your training, you might learn to run a principal components analysis or\n",
    "a factor analysis to ensure they appear to represent \"one underlying\n",
    "factor\" (i.e., sentiment). For now, we can be satisfied with showing\n",
    "acceptable correlations among the ratings and by showing acceptable\n",
    "scale reliability.\n",
    "\n",
    "Next, we proceed to score it as a sale. We use `rowMeans()` to get the\n",
    "average across the five adjectives. Note that `rowMeans()` only accepts\n",
    "a data frame of variables to score, so we must create a data frame. We\n",
    "could do this by manually creating one with `data.frame()` or by using\n",
    "`subset()` and selecting the variable names we want. Either works. I\n",
    "illustrate both ways. In either case, we save the result to `dat` as the\n",
    "new variable, `dat$sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.frame method\n",
    "dat$sentiment <- rowMeans(\n",
    "  data.frame(dat$friendly, dat$inviting, dat$interesting, dat$positive, dat$pleasant)\n",
    ")\n",
    "\n",
    "#subset method\n",
    "dat$sentiment <- rowMeans(\n",
    "  subset(dat, select=c(\"friendly\", \"inviting\", \"interesting\", \"positive\", \"pleasant\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these do the same thing (the `subset()` command returns a data\n",
    "frame equivalent to the one in the first example).\n",
    "\n",
    "Let's check the measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(dat$sentiment)\n",
    "\n",
    "sd(dat$sentiment)\n",
    "\n",
    "ggplot(data=dat, aes(x=sentiment))+\n",
    "  geom_histogram(color=\"black\", fill=\"light blue\", bins=15)+\n",
    "  theme_light()+\n",
    "  scale_x_continuous(name=\"Sentiment\")+\n",
    "  ggtitle(\"Histogram of Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod5_Lab3_-_Experiments_files/figure-markdown_strict/unnamed-chunk-8-1.png)\n",
    "\n",
    "We see that scores tend to weight toward the positive end of the scale.\n",
    "\n",
    "Is this a reliable and valid measure of consumer sentiment? Before we\n",
    "make business decisions with it, we should assess that.\n",
    "\n",
    "We first assess reliability by computing Cronbach's alpha using\n",
    "`alpha()` from the `psych` package. This is done using the same data\n",
    "frame as we used for `rowMeans()` above. Because several packages have\n",
    "an `alpha()` function, we will specify that we want the one from the\n",
    "`psych` package with `psych::alpha()`. The `alpha` command returns a\n",
    "long list of things, but we are only interested in\n",
    "`psych::alpha()$total`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych::alpha(\n",
    "  data.frame(dat$friendly, dat$inviting, dat$interesting, dat$positive, dat$pleasant)\n",
    ")$total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw alpha here is .94, meaning that the scale is an estimated 94%\n",
    "reliable (i.e., 94% of variance is \"true\" variance in sentiment; 6% of\n",
    "the variance is random junk). This is great.\n",
    "\n",
    "The next question is tougher. Are we **validly measuring sentiment and\n",
    "not something else**, such as a tendency to respond positively on\n",
    "surveys? In this case, we don't know. The adjectives seem representative\n",
    "of the positive sentiment we would want to know about (i.e., content\n",
    "validity) and appear good on their face (i.e., face validity). However,\n",
    "we do not have evidence that this sent of adjectives teaks with other\n",
    "indicators of sentiment in this study. Thus, we are accepting on faith\n",
    "that these are a valid measure of sentiment. This is important to keep\n",
    "in mind when interpreting results. It would be ideal if we had a set of\n",
    "\"tried and true\" adjectives from prior studies that we could use that we\n",
    "*know* correlate in ways that show their validity. For now, we just have\n",
    "to trust the measure based on our impression.\n",
    "\n",
    "Vizualize the Data + Run Descriptives\n",
    "=====================================\n",
    "\n",
    "The next step is to visualize the relationships in the data we want to\n",
    "test. We also should examine the descriptive statistics to test our\n",
    "question. In this case, we have randomly assigned participants to logos,\n",
    "so everything about the participants (age, sex, consumption history,\n",
    "etc.) will also be randomly distributed across the three logos. The\n",
    "*only* systematic differences across the three groups was the logo.\n",
    "Thus, any systematic differences in perceived friendliness can be\n",
    "attributed to the logo. This is the beauty of an experimental design. We\n",
    "can easily see the effect of the logo by comparing the groups.\n",
    "\n",
    "In the between-subjects design, we compare means across groups. Thus, we\n",
    "can visualize our data by looking at the distribution of scores across\n",
    "groups. Using `ggplot2` we can use a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=dat, aes(x=logo, y=sentiment, fill=logo))+\n",
    "  geom_boxplot(alpha=.20, color=\"black\")+\n",
    "  geom_jitter(alpha=.5, color=\"black\", fill=\"grey90\", width=.20)+\n",
    "  theme_light()+\n",
    "  scale_y_continuous(name=\"Sentiment\")+\n",
    "  scale_x_discrete(name=\"Logo\")+\n",
    "  scale_fill_discrete(name=\"Logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod5_Lab3_-_Experiments_files/figure-markdown_strict/unnamed-chunk-10-1.png)\n",
    "Annoyingly, we see that we forgot to remove the case with the missing\n",
    "value on the logo, so `ggplot2` created a separate column for it. We can\n",
    "easily fix this by telling ggplot to *include only rows* of `dat` that\n",
    "are \"not missing on the logo variable\" (i.e., `!is.na(dat$logo)`) as\n",
    "follows: `dat[!is.na(dat$logo), ]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=dat[!is.na(dat$logo), ], aes(x=logo, y=sentiment, fill=logo))+\n",
    "  geom_boxplot(alpha=.20, color=\"black\")+\n",
    "  geom_jitter(alpha=.5, color=\"black\", fill=\"grey90\", width=.20)+\n",
    "  theme_light()+\n",
    "  scale_y_continuous(name=\"Sentiment\")+\n",
    "  scale_x_discrete(name=\"Logo\")+\n",
    "  scale_fill_discrete(name=\"Logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod5_Lab3_-_Experiments_files/figure-markdown_strict/unnamed-chunk-11-1.png)\n",
    "We can see in our sample that Logo A and B appear to result in the\n",
    "highest positive sentiment, followed by Logo C. It's not clear whether\n",
    "Logo A or B is better.\n",
    "\n",
    "We can quickly request more detailed statistics using `tapply()`. Recall\n",
    "for `tapply()` accepts four arguments: the first is the variable to\n",
    "analyze, the second is a factor (or list of factors) across which we\n",
    "want to run the analysis, the third is the function we want to use in\n",
    "the analysis, and the rest are arguments to pass along to our function.\n",
    "In this case, we want means of `sentiment` across levels of `logo`, with\n",
    "missing values ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.complete = dat[complete.cases(dat),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Means\n",
    "t(tapply(dat$friendly, dat$logo, mean, na.rm=T))\n",
    "\n",
    "# SDs\n",
    "t(tapply(dat$friendly, dat$logo, sd, na.rm=T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also make a table and wrap each `tapply()` call in a `round()`\n",
    "command to make a pleasant table of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Mean and SD Table\n",
    "tab.1 <- rbind(\n",
    "  round(tapply(dat$sentiment, dat$logo, mean, na.rm=T), 2),\n",
    "  round(tapply(dat$sentiment, dat$logo, sd, na.rm=T), 2)\n",
    ")\n",
    "\n",
    "tab.1 <- t(tab.1)\n",
    "colnames(tab.1) <- c(\"M\", \"SD\")\n",
    "tab.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo A and B look close together and ahead of Logo C. Of course, that is\n",
    "just in our sample, so it is unclear whether we can infer there are real\n",
    "differences in the population. For that, we need our inferential tests.\n",
    "\n",
    "Inferential Test for Two Groups\n",
    "===============================\n",
    "\n",
    "In some experiments, you will have two groups to compare. For example,\n",
    "in the Module 3 lab on causal claims, we explored such an experiment. In\n",
    "others, you will have three or more groups. Although our overarching\n",
    "example is three groups, let's review this and pretend for a while that\n",
    "you were *only* comparing logos A and B to illustrate the two-group\n",
    "example.\n",
    "\n",
    "Whenever you compare (means of) two groups, you conduct an independent\n",
    "samples *t*-test. You can also use regression (run a regression\n",
    "predicting your outcome from your two-group predictor), but the *t*-test\n",
    "is commonly done and is illustrated here.\n",
    "\n",
    "Recall that Logo A did better *in our sample*, but is this a *real*\n",
    "difference in the population or just an artifact of random chance (due\n",
    "to reliance on random assignment to groups)?\n",
    "\n",
    "Recall that the null hypothesis always says that the **effect is absent\n",
    "in the population** and that the sample result is an artifact of random\n",
    "chance. In symbols, this means that the difference between the group\n",
    "averages is exactly zero in the population:\n",
    "\n",
    "$$H_0:\\ \\mu_A - \\mu_B = 0$$\n",
    "\n",
    " Remember that *μ* refers to the population average, so this is saying\n",
    "that the population difference is exactly zero. Any difference observed\n",
    "in our sample is therefore due to random chance.\n",
    "\n",
    "We run our *t*-test to consider this possibility.\n",
    "\n",
    "Recall that a *t*-test compares the size of the *observed difference*\n",
    "($\\bar{x}_{1}-\\bar{x}_{2}$) against the value in the null hypothesis\n",
    "(zero), divided by what is typically expected by chance:\n",
    "\n",
    "$$t=\\frac{result - null }{chance}$$  \n",
    "\n",
    "The top of the faction is key here. The more the data \"disagree\" with\n",
    "the null, the larger the *t*-value and hence more evidence for a logo\n",
    "effect. However, we also know some differences occur due to random\n",
    "chance, so we divide by a measure of that that to take it into account.\n",
    "When all is said and done, therefore, a large *t*-value tells you that\n",
    "the effect is considerably larger than expected by chance. That would be\n",
    "evidence for a real logo effect.\n",
    "\n",
    "How can we run our test? The default in R is to run the \"Welch\" version\n",
    "of the test. This version of the test does *not* make any assumptions\n",
    "about the variances of the two groups.\n",
    "\n",
    "$$t'=\\frac{result - null }{chance}=\\frac{(\\bar{x}_{1}-\\bar{x}_{2}) - 0 }{\\sqrt{\\frac{\\hat{\\sigma}_1^2}{n_{1}}+\\frac{\\hat{\\sigma}_2^2}{n_{2}}}}$$\n",
    "\n",
    "There's no need to worry about the equation; it just implements what we\n",
    "said above. Notice that the bottom includes the sample variances of the\n",
    "two groups ($\\hat{\\sigma}_1^2$ and $\\hat{\\sigma}_2^2$ .... i.e.,\n",
    "standard deviations squared). These are kept separate in the Welch\n",
    "*t*-test, meaning that it's OK if they are different from each other.   \n",
    "\n",
    "How is this done via R?\n",
    "\n",
    "First, we need a data frame that restricts `logo` to the two groups of\n",
    "interest. In this case, I make `dat2` that contains only rows for which\n",
    "`dat$logos` is Logo A or (symbol for \"or\" is `|`) Logo B (for the\n",
    "purposes of example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2 <- dat[(dat$logo == \"Logo A\" | dat$logo==\"Logo B\"), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R code for the *t* test looks similar to the code for a regression.\n",
    "Either of the following would work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test(dat2$sentiment ~ dat2$logo)\n",
    "\n",
    "t.test(sentiment ~ logo, data=dat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't show you this one yet, but a third option is to ditch the\n",
    "`sentiment ~ logo` notation and simply input two vectors of sentiments\n",
    "representing the two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test(dat2$sentiment[dat2$logo==\"Logo A\"], dat2$sentiment[dat2$logo==\"Logo B\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all three cases, you get the same result.\n",
    "\n",
    "The *t*-value of 0.38 tells us that the difference is 0.38 times the\n",
    "size of what one would typically expect by chance. This gives us our\n",
    "*p*-value. We see here that the *p*-value is .70 so by definition, we\n",
    "could get a result this big in our sample 70% of the time when the null\n",
    "hypothesis that there is *NO* difference is true. This is not\n",
    "statistically significant (*p* must be below .05 or 5%). We must accept\n",
    "the null hypothesis that this could plausibly be due to random chance. I\n",
    "don't have enough evidence to declare a real difference between Logo A\n",
    "and Logo B.\n",
    "\n",
    "#### Another Option: Student's *t*\n",
    "\n",
    "Note that there is another version of the *t*-test, Student's *t*, which\n",
    "is the same but it uses a form of averaging of the two sample variances:\n",
    "\n",
    "$$t=\\frac{result - null }{chance}= \\frac{(\\bar{x}_{1}- \\bar{x}_{2}) - 0 }{\\sqrt{\\hat{\\sigma}_p^2 (\\frac{1}{n_{1}}+\\frac{1}{n_{2}}))}}$$\n",
    " In this case, the $\\hat{\\sigma}_p^2$ is a weighted average of the\n",
    "two sample variances. If you cared for the equation for that (which you\n",
    "probably don't), it looks like this:\n",
    "\n",
    "$$\\hat{\\sigma}_p^2 = \\frac{\\left( n_1 - 1 \\right)\\hat{\\sigma}_1^2+\\left( n_2-1 \\right)\\hat{\\sigma}_2^2}{\\left( n_1 - 1 \\right)+\\left( n_2-1 \\right)}$$\n",
    " Essentially, this is a sophisticated form of averaging\n",
    "$\\hat{\\sigma}_1^2$ and $\\hat{\\sigma}_2^2$ so that the one from the\n",
    "larger group has more influence over the final answer.\n",
    "\n",
    "Long story short: this may not be the best test, as it *requires* the\n",
    "variances (*σ*<sub>1</sub><sup>2</sup> and *σ*<sub>2</sub><sup>2</sup> )\n",
    "of the two groups to be equal at the population level. Since we can\n",
    "never know anything about the population, we can never know if this\n",
    "assumption is met. We can test for it, but that has its own\n",
    "complications. Further, recent advice (Delacre, Lakens, & Leys, 2017)\n",
    "indicates that the Welch's *t* (done above) is better under real-world\n",
    "conditions. So, we will skip this version. If you wanted to run it, you\n",
    "would simply add the equal-variance assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test(dat2$sentiment ~ dat2$logo, var.equal=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only other thing to know about the difference between the two tests\n",
    "is that Welch's *t*-test is often slightly less powerful (we act as\n",
    "though it were based on a smaller sample than it actually was). The\n",
    "rationale for that goes beyond the scope of this tutorial, but\n",
    "essentially we are adding some uncertainty into our result because we\n",
    "are using two sample-based numbers ($\\hat{\\sigma}_1^2$ and\n",
    "$\\hat{\\sigma}_2^2$) in the bottom of our equation, both of which are\n",
    "prone to error. It's worth it, however, as you don't make as many\n",
    "assumptions about the data. Still, in cases of very small samples (or\n",
    "very uneven sized groups), Student's *t* test is an acceptable backup.\n",
    "\n",
    "#### Conclusions\n",
    "\n",
    "We need to tell our team that Logo A and Logo B are statistically tied:\n",
    "we have no evidence for any real difference between them at the\n",
    "population level. But what does that mean?\n",
    "\n",
    "Before we move on: remember that **any** time a result is not\n",
    "significant, it means that either\n",
    "\n",
    "1.  The null hypothesis is true (i.e., the two groups are **exactly**\n",
    "    equal in the population)\n",
    "2.  The effect is smaller than we have power to detect\n",
    "\n",
    "In fact, given our sample size, it's possible that there may still be an\n",
    "logo effect...and we simply missed it. Given our sample size, what can\n",
    "we actually *say* about the difference? This is given for us in the\n",
    "*t*-test output with a 95% Confidence Interval.\n",
    "\n",
    "Notice in the middle of the *t*-test output that a 95% CI is given to\n",
    "you: \\[-0.55 to 0.82\\]. This means that we are 95% confident that the\n",
    "true difference between the groups is somewhere between a half a point\n",
    "*lower* for Logo A and nearly a full point *higher* for Logo A. Thus, it\n",
    "is obvious this is non-significant (since a difference of *zero* is\n",
    "right in the middle of that range of plausible values).\n",
    "\n",
    "It is up to our subject matter expertise to interpret this. On a 1-10\n",
    "satisfaction scale, we are confident that the they are within about .50\n",
    "to .80 points of each other. That's not a very big difference in my\n",
    "subjective opinion. Thus, we can reasonably state that there is not a\n",
    "large difference between our groups, but it is possible there is still a\n",
    "small difference (specifically: between a half a point *lower* for Logo\n",
    "A and nearly a full point *higher* for Logo A).\n",
    "\n",
    "What if we wanted a more precise answer with a tighter range? Well, we\n",
    "would need a bigger sample size. A bigger sample has less error and\n",
    "therefore a more precise CI. If our higher ups wanted a more precise\n",
    "answer, we would need to re-run the survey with a larger sample.\n",
    "\n",
    "#### If Units Are Meaningless\n",
    "\n",
    "In this case, a 1-10 satisfaction scale has somewhat meaningful units.\n",
    "In some situations, this is not the case. In such situations, you can use the\n",
    "Cohen's *d* effect size statistic.\n",
    "\n",
    "Recall from an earlier module that this quantifies the size of the\n",
    "difference using a standard unit. Technically, that unit is standard\n",
    "deviations:   \n",
    "\n",
    "$d = \\frac{difference}{SD} = \\frac{\\bar{x}_{1}-\\bar{x}_{2}}{\\sigma}$.   \n",
    "\n",
    "If you don't care about formulas, you simply need to know that it\n",
    "rephrases the size of the difference using a common unit with a known\n",
    "scale to it:\n",
    "\n",
    "|        | d Value          | Meaning  |\n",
    "| ------------- |:-------------:|-----------:|\n",
    "| 1.  | 0 - 0.2 | Negligible |\n",
    "| 2.  | 0.2 - 0.5     |  Small |\n",
    "| 3. | 0.5 - 0.8      |  Medium |\n",
    "| 4. | 0.80 +      |  Large |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this in R, we use the `effsize` package, which contains a\n",
    "`cohen.d()` function. Because both the `effsize` and `psych` packages\n",
    "contain functions with that name, we specify here that it comes from the\n",
    "`effsize` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effsize::cohen.d(dat2$sentiment ~ dat2$logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the difference between them in the sample was classified as\n",
    "\"negligible.\" Further, this gives you the same 95% CI from earlier, but\n",
    "in Cohen's *d* units instead of \"points\" on the satisfaction scale.\n",
    "Neat! I won't use this here, given that we can interpret points on a\n",
    "1-10 satisfaction scale pretty easily. Still, this is a good tool to\n",
    "have.\n",
    "\n",
    "#### Final Report\n",
    "\n",
    "Let's see how we might summarize our final conclusions. Note that this\n",
    "would be written differently depending on who the audience is (i.e.,\n",
    "board members, other data scientists). I present the most frank\n",
    "(non-hyped-up) version of the analysis here.\n",
    "\n",
    "First, I would begin with a visualization of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=dat2[!is.na(dat2$logo), ], aes(x=logo, y=sentiment, fill=logo))+\n",
    "  geom_boxplot(alpha=.20, color=\"black\")+\n",
    "  geom_jitter(alpha=.5, color=\"black\", fill=\"grey90\", width=.20)+\n",
    "  theme_light()+\n",
    "  scale_y_continuous(name=\"Sentiment\")+\n",
    "  scale_x_discrete(name=\"Logo\")+\n",
    "  scale_fill_discrete(name=\"Logo\")+\n",
    "  ggtitle(\"Sentiment: Logo A vs. Logo B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod5_Lab3_-_Experiments_files/figure-markdown_strict/unnamed-chunk-20-1.png)\n",
    "\n",
    "Second, I present the results. Generally, I try to state what we\n",
    "observed first, quantifying the size of the observed results. Second, I\n",
    "try to state something about whether the effect is significant, using\n",
    "common language where possible. Third, I try to present some measure of\n",
    "uncertainty (e.g., our CI), interpreting in plain language as much as\n",
    "possible. Here is a paragraph possibility:\n",
    "\n",
    "> The average sentiment for Logo A was slightly higher (*M* = 8.57, *SD*\n",
    "> = 1.27) than the average sentiment for Logo B (*M* = 8.44, *SD* =\n",
    "> 1.51). This was a small difference...only 0.13 points on the 1-10\n",
    "> scale. Given our sample size, this was well within what would be\n",
    "> expected by random chance (i.e., not statistically significant; *t* =\n",
    "> 0.38, *p* = 0.70). Thus, they were statistically tied.\n",
    "\n",
    "> It is possible that a small sentiment difference may still exist\n",
    "> between the two logos. We are 95% confident that the difference is\n",
    "> somewhere between 0.80 points higher for Logo A and 0.55 pionts higher\n",
    "> for Logo B. So, if they do differ, they are likely very close to one\n",
    "> another; however, we woudl need a larger sampel to say something more\n",
    "> precise than that.\n",
    "\n",
    "#### Addendum: Power Considerations\n",
    "\n",
    "We are done with the analysis. However, it's worth briefly chatting\n",
    "about power. Did our study have much ability to detect a difference\n",
    "between Logo A and Logo B in the first place? Was it set up to fail?\n",
    "\n",
    "Recall that power depends on both sample size and effect size. Large\n",
    "samples can detect small effects. Smaller samples cannot detect small\n",
    "effects.\n",
    "\n",
    "We simply feed information about our study (our sample size and request\n",
    "for 80% power) to the `pwr.t2n.test()` command from the `pwr` package.\n",
    "It returns to us the smallest effect we could reliably detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ask R for our sample sizes\n",
    "table(dat2$logo)\n",
    "\n",
    "# Feed information inot pwr.t2n.test()\n",
    "pwr.t2n.test(n1=33, n2=32, power=.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha. We could **ONLY** reliably detect effects as small as *d* = .71 ...\n",
    "a \"moderate-large\" effect. So, this reinforces our conclusion above...\n",
    "we would have detected a moderate-large difference between Logo A and B,\n",
    "but if there was a smaller difference, we were prone to miss it.\n",
    "\n",
    "Why bring this up? Because I want to remind you that a non-significant\n",
    "result *does not mean the effect was exactl zero*. It would be a mistake\n",
    "to say \"there was NO difference between Logo A and Logo B.\" Instead, as\n",
    "we stated above, we should say \"there was not a large difference between\n",
    "Logo A and Logo B.\" This reinforces that, but you wouldn't need to\n",
    "report this.\n",
    "\n",
    "However, this *would* be worth doing prior to running the study as we\n",
    "plan our sample size. What if we wanted to be able to detect a smaller\n",
    "difference, say *d* = .20 with our 80% power? In this case, enter the\n",
    "*d* value and leave out the *n* values from the power command. I use the\n",
    "`pwr.t.test()` variant which makes the group sizes equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwr.t.test(d=.20, power=.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we would need nearly 393 people *per logo* to detect\n",
    "small differences. If we were planning the study from the start, we\n",
    "might iteratively explore several possible sample sizes to find the\n",
    "optimal balance between sample size (i.e., cost of the study) and\n",
    "ability to detect an effect.\n",
    "\n",
    "Example 1: Inferential Test for Three Groups\n",
    "============================================\n",
    "\n",
    "The previous example considered only two groups: Logo A and Logo B.\n",
    "However, we had three logos in our study.\n",
    "\n",
    "When three or more groups are present, we switch from a *t*-test to\n",
    "*ANOVA* (ANalysis Of VAriance).\n",
    "\n",
    "The ANOVA is a two stage analysis. In the first stage, we test whether\n",
    "we can the logo mattered at *all* (i.e., null hypothesis says there were\n",
    "*no* differences:\n",
    "*H*<sub>0</sub> : *μ*<sub>*A*</sub> = *μ*<sub>*B*</sub> = *μ*<sub>*C*</sub>).\n",
    "If that comes up significant, we can systematically compare the logos.\n",
    "\n",
    "In this example, I assume you are familiar with ANOVA. An overview of\n",
    "the math behind ANOVA goes beyond the scope of this lab. Essentially,\n",
    "ANOVA computes the *F* statistic, which yields a *p*-value for the null\n",
    "hypothesis that all the groups are equal. If we can reject that null\n",
    "hypothesis, it means the logo is doing something. I illustrate this\n",
    "next.\n",
    "\n",
    "In R, ANOVA is relatively easy to test. I illustrate the base R version\n",
    "in this lab. In base R, we use the `aov()` command to run the model,\n",
    "which functions the same as the `t.test()` command. We first save the\n",
    "result and then request a `summary()` of the model. **Note that this\n",
    "will give incorrect results if `dat$logo` is not a factor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 <- aov(dat$sentiment ~ dat$logo)\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step tests the null hypothesis that *all the group means are\n",
    "equall* ... in other words that the logo doesn't matter **at all**. In\n",
    "this case, the *p* value is 6.25e-06 (aka 0.00000625). This is highly\n",
    "statistically significant (*p* &lt; .05). By definition, this means that\n",
    "we could only get a logo effect this big 0.000625% of the time by random\n",
    "chance alone. So we are confident the logo is doing something.\n",
    "\n",
    "Why did we bother with the test described above? Consider what would\n",
    "happen had we done several *t*-tests. There were three possible\n",
    "*t*-tests we could have conducted, each of which has the possibility of\n",
    "giving us a false positive. If we were to run *many* tests, we would\n",
    "likely stumble across a false-positive eventually. Thus, we appreciate\n",
    "the ability to conduct a **single** overall test to tell us our logo is\n",
    "doing something.\n",
    "\n",
    "The next step is to figure out where the differences are. Recall we made\n",
    "a table of results earlier with `tapply()`.\n",
    "\n",
    "To avoid the issue described above, we use a special test (there are\n",
    "many; I cover Tukey's HSD here) that takes into account the number of\n",
    "comparisons we want to make. We wish to make 3 comparisons:\n",
    "\n",
    "1.  A vs. B\n",
    "2.  A vs. C\n",
    "3.  B vs. C\n",
    "\n",
    "Tukey's test knows how many comparisons we want to make. It inflates our\n",
    "*p*-values on each test, making it harder to detect a difference for any\n",
    "one comparison. This *reduces* our likelihood of making a false positive\n",
    "on any one comparison, counteracting the problem of running multiple\n",
    "tests. Neat!\n",
    "\n",
    "It is easy to run. Simply feed your ANOVA model into: `TukeyHSD()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TukeyHSD(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note here:\n",
    "\n",
    "1.  Group A vs. B = not significant (*p* = .92)\n",
    "2.  Group A vs. C = highly significant (*p* = 0.0000257)\n",
    "3.  Group B vs C = highly significant (*p* = 0.0001337)\n",
    "\n",
    "The output above also gave us difference scores + 95% CIs. Very handy:\n",
    "\n",
    "1.  Logo B was 0.13 points lower than Logo A. We are 95% confident the\n",
    "    true difference is between -0.94 and 0.68. This includes zero, so we\n",
    "    cannot declare a difference between Logo B and Logo A.\n",
    "\n",
    "2.  Logo C was 1.58 points lower than Logo A. We are 95% confident the\n",
    "    true difference is between -2.37 and -0.78.\n",
    "\n",
    "3.  Logo C was 1.44 points lower than Logo B. We are 95% confident the\n",
    "    true difference is between -2.25 and -0.64.\n",
    "\n",
    "As in the *t*-test example above, we see the confidence intervals are\n",
    "helpful. If we wanted more precise CIs, we need a larger sample.\n",
    "\n",
    "As a reminder, if we needed Means and SDs, we could easily use\n",
    "`tapply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Mean and SD Table\n",
    "tab.1 <- rbind(\n",
    "  round(tapply(dat$sentiment, dat$logo, mean, na.rm=T), 2),\n",
    "  round(tapply(dat$sentiment, dat$logo, sd, na.rm=T), 2)\n",
    ")\n",
    "\n",
    "tab.1 <- t(tab.1)\n",
    "colnames(tab.1) <- c(\"M\", \"SD\")\n",
    "tab.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Report\n",
    "\n",
    "A report might look as follows:\n",
    "\n",
    "> The average sentiment on a 1-10 scale is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An ANOVA revealed a signifiant effect of logo on sentiment (*F* =\n",
    "> 13.61, *p* &lt; .001). As shown in the table above, Logos A and B were\n",
    "> similar in sentiment, while Logo C had reduced sentiment. The pattern\n",
    "> of results was such that Logos A and B were statistically tied, and\n",
    "> both performed better than Logo C.\n",
    "\n",
    "Given the number of results, one could then report the CIs or *p*-values\n",
    "above in a table or in paragraph form. It is possible to go into the\n",
    "same level of detail as for the *t*-test, but people usually don't, as\n",
    "it becomes exponentially more complicated with all the comparisons going\n",
    "on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
